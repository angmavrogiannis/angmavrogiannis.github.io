<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Projects</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><div class="menu-head">Angelos Mavrogiannis</div></div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="projects.html" class="current">Projects</a></div>
<div class="menu-item"><a href="courses.html">Courses</a></div>
<div class="menu-item"><a href="music.html">Music</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Projects</h1>
</div>
<h2>Vehicle Behavior Prediction</h2>
<table class="imgtable"><tr><td>
<img src="images/lstm.png" alt="LSTM Architecture" width="300px" />&nbsp;</td>
<td align="left"><p>Having joined Intelligent Control Lab at the Robotics Institute on spring 2019, I started working on vehicle behavior prediction, advised by Prof. <b><a href="http://www.cs.cmu.edu/~cliu6/">Changliu Liu</a></b>. Precisely, I labeled vehicle trajectory data taken from the <b><a href="https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm">Next Generation Simulation (NGSIM)</a></b> dataset based on their maneuver attempted or executed during the next 3 seconds (keep current lane <i> turn left </i> turn right). I formulated a multiclass classification problem of predicting the intention/maneuver of a vehicle for a predetermined time horizon using semantic features, such as vehicle coordinates and velocity, and implemented a Long Short-Term Memory (LSTM) network that outputs the probability of each class at every time step.</p>
</td></tr></table>
<h2>Robot Design</h2>
<table class="imgtable"><tr><td>
<img src="images/oswald.png" alt="Oswald" width="300px" />&nbsp;</td>
<td align="left"><p>Drawing inspiration from penguins, I collaborated with a team of students to design, manufacture and test an underwater penguin robot for the course <b><a href="https://www.andrew.cmu.edu/user/amj1/classes/robotdesign.html">Robot Design &amp; Experimentation</a></b> taught by Prof. <b><a href="http://www.andrew.cmu.edu/user/amj1/">Aaron Johnson</a></b>. We came up with a ball-and-socket motion transmission mechanism for the movement of the flippers, fabricated a rib-and-spar body using 3D printers and laser-cutters and used Arduino for the controls. A video we created for our project, containing interviews and testing footage can be found <b><a href="https://www.youtube.com/watch?v=3IRqu0saWe8">here</a></b>.</p>
</td></tr></table>
<h2>Game Design</h2>
<table class="imgtable"><tr><td>
<img src="images/sticky_man.jpeg" alt="Sticky-Man game" width="300px" />&nbsp;</td>
<td align="left"><p>Using <b><a href="https://www.opengl.org/">OpenGL</a></b> on C<tt></tt>, we implemented a 2D fighting game for the course <b><a href="https://www.meche.engineering.cmu.edu/education/courses/24-780.html">Engineering Computation</a></b>, taught by Prof. <b><a href="https://engineering.cmu.edu/directory/bios/gomez-nestor.html">Nestor Gomez</a></b>. On the single player mode, the user controls a sticky-man figure and fights vs an AI agent. On the multiplayer mode, two users control their own sticky-man figure and fight until one of them is eliminated. The sticky-man figure can take a different set of states: gun mode (ranged), knife mode (melee), fight mode (melee). Code implementation can be found <b><a href="https://github.com/angmavrogiannis/24-780B-Engineering-Computation/tree/master/Sticky-Man">here</a></b>.</p>
</td></tr></table>
<h2>Occluded Object Pose Estimation </h2>
<table class="imgtable"><tr><td>
<img src="images/object.png" alt="Egg-shaped object" width="300px" />&nbsp;</td>
<td align="left"><p>While manipulating objects in activities of daily living, we come across a problem where objects
are quite often severely occluded from the egocentric viewpoint making it difficult for the object
of interest to be tracked. Inspired by this problem, we collected a synthetic dataset of <b><a href="https://www.shadowrobot.com/products/dexterous-hand/">manipulator</a></b> postures and object poses in <b><a href="https://gym.openai.com/">OpenAI Gym</a></b> and mapped changes in hand pose to object displacements in order to track occluded objects. Our approach consists of a Multilayer Perceptron that takes as input the joint angles of the manipulator and outputs the position and rotation of the object. This project was a part of the course <b><a href="http://www.cs.cmu.edu/afs/cs/academic/class/16741-s07/www/index.html">Mechanics of Manipulation</a></b>, taught by Prof. <b><a href="https://www.cs.cmu.edu/~mason/">Matthew Mason</a></b>.</p>
</td></tr></table>
<div id="footer">
<div id="footer-text">
Page generated 2019-10-22 17:31:29 EDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
