<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-151329879-1"></script>
<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'UA-151329879-1');
</script>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Projects</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="projects.html" class="current">Projects</a></div>
<div class="menu-item"><a href="coursework.html">Coursework</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="music.html">Music</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Projects</h1>
</div>
<h2>B-GAP: Behavior-Guided Action Prediction for Autonomous Navigation</h2>
<table class="imgtable"><tr><td>
<img src="images/aggro.gif" alt="bgap" width="300px" />&nbsp;</td>
<td align="left"><p>We present an algorithm for behaviorally-guided action prediction and local navigation for autonomous driving in dense traffic scenarios. Our approach classifies the driver behavior of other vehicles or road-agents (aggressive or conservative) and considers that information for decision-making and safe driving. We present a behavior-driven simulator that can generate trajectories corresponding to different levels of aggressive behaviors, and we use our simulator to train a reinforcement learning policy using a multilayer perceptron neural network. We use our reinforcement learning-based navigation scheme to compute safe trajectories for the ego-vehicle accounting for aggressive driver maneuvers such as overtaking, over-speeding, weaving, and sudden lane changes. We have integrated our algorithm with the OpenAI gym-based &lsquo;&lsquo;Highway-Env&rsquo;&rsquo; simulator and demonstrate the benefits of our navigation algorithm in terms of reducing collisions by 3.25−26.90% and handling scenarios with 2.5× higher traffic density. Project supervised by Prof. <b><a href="https://scholar.google.com/citations?user=X08l_4IAAAAJ&amp;hl=en&amp;oi=ao">Dinesh Manocha</a></b> at <b><a href="https://gamma.umd.edu/">gamma group</a></b>, <b><a href="https://www.umd.edu">University of Maryland, College Park</a></b>. [<b><a href="https://arxiv.org/abs/2011.03748">arXiv</a></b>] [<b><a href="https://gamma.umd.edu/bgap">project page</a></b>] [<b><a href="https://github.com/angmavrogiannis/B-GAP-Behavior-Guided-Action-Prediction-for-Autonomous-Navigation">code</a></b>] [<b><a href="https://youtu.be/UPkHs_1kz9k">video</a></b>]</p>
</td></tr></table>
<h2>Human Driver Behavior Classification from Partial Trajectory Observation</h2>
<table class="imgtable"><tr><td>
<img src="images/ngsim_visualizer.png" alt="NGSIM Visualizer" width="300px" />&nbsp;</td>
<td align="left"><p>As autonomous vehicles are being tested on public roads, they must be able to share the road safely with human-driven vehicles. To ensure safety, autonomous vehicles must be capable of accurately estimating human drivers&rsquo; intentions and their future trajectories. While there has been extensive research in this area, most of the existing approaches do not take into account the individual drivers&rsquo; personalities and the patterns these personalities reflect on the trajectories of the vehicles. We tackle this issue by proposing a novel method of extracting high-level features from raw vehicle trajectory data and classifying drivers into behavioral classes based on their level of aggressiveness. We demonstrate how the identification of a driver's behavior improves the accuracy of the short-term trajectory prediction problem by introducing a prior knowledge on their behavior. Thesis supervised by Prof. <b><a href="https://www.cs.cmu.edu/~cliu6/">Changliu Liu</a></b> at the <b><a href="http://icontrol.ri.cmu.edu/">Intelligent Control Lab</a></b> in the <b><a href="https://www.ri.cmu.edu/">Robotics Institute</a></b> of <b><a href="https://www.cmu.edu/">Carnegie Mellon University</a></b>. [<b><a href="https://www.researchgate.net/publication/345780499_Human_Driver_Behavior_Classification_from_Partial_Trajectory_Observation">thesis</a></b>] [<b><a href="https://github.com/angmavrogiannis/Human-Driver-Behavior-Classification-from-Partial-Trajectory-Observation">code</a></b>] [<b><a href="https://www.youtube.com/watch?v=ldNasL2I--A">video</a></b>] [<b><a href="https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm">dataset</a></b>]</p>
</td></tr></table>
<h2>Autonomous Vehicle Controller Design</h2>
<table class="imgtable"><tr><td>
<img src="images/route.png" alt="CMU buggy route" width="300px" />&nbsp;</td>
<td align="left"><p>As part of the course <b><a href="https://www.meche.engineering.cmu.edu/education/courses/24-677.html">Linear Control Systems</a></b>, taught by Prof. <b><a href="https://www.meche.engineering.cmu.edu/directory/bios/zhao-ding.html">Ding Zhao</a></b>, I designed a longitudinal and a lateral controller using different techniques to track the route of an autonomous <b><a href="https://www.cmu.edu/buggy/">buggy</a></b> vehicle around the CMU campus. A buggy simulator on python was used for getting the required response plots and tuning the parameters of each control method used. The longitudinal motion is controlled by a PID controller, while the lateral motion is controlled using:<br /></p>
<ul>
<li><p>A PID Controller</p>
</li>
<li><p>Pole Placement</p>
</li>
<li><p>Model Predictive Control</p>
</li>
<li><p>Kalman Filter</p>
</li>
</ul>
<p>The autonomous vehicle is required to achieve certain performance criteria, such as a minimum time to complete the route and an average and maximum deviation from the reference trajectory. In the end, a race was held and we competed with each other based on these criteria. [<b><a href="https://github.com/angmavrogiannis/24677-Linear-Control-Systems">code</a></b>]</p>
</td></tr></table>
<h2>Robot Design</h2>
<table class="imgtable"><tr><td>
<img src="images/oswald.png" alt="Oswald" width="300px" />&nbsp;</td>
<td align="left"><p>Drawing inspiration from penguins, I collaborated with a team of students to design, manufacture and test an underwater penguin robot for the course <b><a href="https://www.andrew.cmu.edu/user/amj1/classes/robotdesign.html">Robot Design &amp; Experimentation</a></b> taught by Prof. <b><a href="https://www.andrew.cmu.edu/user/amj1/">Aaron Johnson</a></b>. We came up with a ball-and-socket motion transmission mechanism for the movement of the flippers, fabricated a rib-and-spar body using 3D printers and laser-cutters and used Arduino for the controls. Besides contributing to the overall design and manufacturing process of the robot, I developed an underwater simulator on <b><a href="http://gazebosim.org/">gazebo</a></b> with the model of our constructed robot and used it to adjust the control parameters and perform tests before submerging the robot in the water. [<b><a href="docs/Oswald Final Project Report.pdf">report</a></b>] [<b><a href="docs/Mavrogiannis Final Executive Summary.pdf">executive summary</a></b>] [<b><a href="https://www.youtube.com/watch?v=3IRqu0saWe8">video</a></b>]</p>
</td></tr></table>
<h2>Game Design</h2>
<table class="imgtable"><tr><td>
<img src="images/sticky_man2.jpeg" alt="Sticky-Man game" width="300px" />&nbsp;</td>
<td align="left"><p>Using <b><a href="https://www.opengl.org/">OpenGL</a></b> on C++, we implemented a 2D fighting game for the course <b><a href="https://www.meche.engineering.cmu.edu/education/courses/24-780.html">Engineering Computation</a></b>, taught by Prof. <b><a href="https://engineering.cmu.edu/directory/bios/gomez-nestor.html">Nestor Gomez</a></b>. On the single player mode, the user controls a sticky-man figure and fights vs an AI agent. On the multiplayer mode, two users control their own sticky-man figure and fight until one of them is eliminated. The sticky-man figure can take a different set of states: gun mode (ranged), knife mode (melee), fight mode (melee). I created a menu for the game, as well as the background environment, enabling the players to actively interact with it (e.g. climbing stairs, jumping towards different floor levels). I also implemented a simple AI algorithm for the enemy player on the single player mode. [<b><a href="https://github.com/angmavrogiannis/24-780B-Engineering-Computation/tree/master/Sticky-Man">code</a></b>] [<b><a href="https://www.youtube.com/watch?v=kKTP6xleipg">video</a></b>]</p>
</td></tr></table>
<h2>Occluded Object Pose Estimation </h2>
<table class="imgtable"><tr><td>
<img src="images/object.png" alt="Egg-shaped object" width="300px" />&nbsp;</td>
<td align="left"><p>While manipulating objects in activities of daily living, we come across a problem where objects
are quite often severely occluded from the egocentric viewpoint making it difficult for the object
of interest to be tracked. Inspired by this problem, we collected a synthetic dataset of <b><a href="https://www.shadowrobot.com/products/dexterous-hand/">manipulator</a></b> postures and object poses in <b><a href="https://gym.openai.com/">OpenAI Gym</a></b> and mapped changes in hand pose to object displacements in order to track occluded objects. Our approach consists of a Multilayer Perceptron that takes as input the joint angles of the manipulator and outputs the position and rotation of the object. I worked on developing the neural network model and encoding the necessary input for the training process. I combined manipulator pose changes and previous object and manipulator positions into a vector, which gets fed to the network. The output of the network consists of the future positional coordinates and the quaternior (for rotation) of the predicted pose. This project was a part of the course <b><a href="https://www.cs.cmu.edu/afs/cs/academic/class/16741-s07/www/index.html">Mechanics of Manipulation</a></b>, taught by Prof. <b><a href="https://www.cs.cmu.edu/~mason/">Matthew Mason</a></b>. [<b><a href="docs/16-741 Term Project Report.pdf">report</a></b>]</p>
</td></tr></table>
<h2>Genetic-Algorithm-based Optimization Framework </h2>
<table class="imgtable"><tr><td>
<img src="images/gripper.png" alt="gripper-object forces" width="300px" />&nbsp;</td>
<td align="left"><p>For my undergraduate diploma thesis, I developed a framework on Visual Basic that receives mathematical expressions as input, analyzes them using a suitable parser, and optimizes them with genetic algorithms. The parser allows the input of the expressions in string format and distinguishes the variables, the parameters and the operational symbols. Besides the equations, the user can choose between a set of genetic algorithms for the optimization, as well as the hyperparameters. The implemented software was tested and validated on two applications: the minimization of the forces applied onto an object grasped by a robotic arm and the maximization of the stiffness of a cantilever beam. [<b><a href="https://github.com/angmavrogiannis/Diploma-Thesis">code</a></b>] [<b><a href="docs/DT_M_Mavrogiannis_Angelos_6387.pdf">thesis (in greek, abstract in english)</a></b>]</p>
</td></tr></table>
<h2>Computational Robotics Project</h2>
<table class="imgtable"><tr><td>
<img src="images/kuka_robot.jpg" alt="KUKA robot" width="300px" />&nbsp;</td>
<td align="left"><p>Forward and inverse kinematics constitute some of the most fundamental concepts in robotics. As part of an undergraduate robotics course, I chose a 6-degree-of-freedom industrial robot, namely <b><a href="https://www.kuka.com/en-us/products/robotics-systems/industrial-robots/kr-agilus?fbclid=IwAR1Z8iihzDCxpbKhjGFO33Bd0a9jV_eUrqWtp7ri1ybRV-G9Zaz1ItZgnGU">KUKA KR 6 R700 sixx WP</a></b>, upon which I developed software on MATLAB that does the following:
<br /></p>
<ul>
<li><p>Calculates the Denavit-Hartenberg parameters of the robot.</p>
</li>
<li><p>Implements the forward kinematics.</p>
</li>
<li><p>Implements the inverse kinematics.</p>
</li>
<li><p>Computes the Jacobian matrix.</p>
</li>
</ul>
<p>Finally, I applied the software to a trajectory planning application, combining linear and quadratic interpolation for two points in space. [<b><a href="https://github.com/angmavrogiannis/MEA-KY3-Robotics">code</a></b>]</p>
</td></tr></table>
<h2>2D Animation</h2>
<table class="imgtable"><tr><td>
<img src="images/pumpkins.jpeg" alt="Pumpkins" width="300px" />&nbsp;</td>
<td align="left"><p>For the individual project of the course <b><a href="https://www.meche.engineering.cmu.edu/education/courses/24-780.html">Engineering Computation</a></b> (taught by Prof. <b><a href="https://engineering.cmu.edu/directory/bios/gomez-nestor.html">Nestor Gomez</a></b>), I implemented a halloween-themed demo using <b><a href="https://www.opengl.org/">OpenGL</a></b> on C++. The demo includes 2D animation made with <b><a href="http://ysflight.in.coocan.jp/programming/fssimplewindow/e.html">this</a></b> rendering framework and music that I wrote, recorded and synchronized with the animation transitions. [<b><a href="https://github.com/angmavrogiannis/24-780B-Engineering-Computation/tree/master/Demo1">code</a></b>] [<b><a href="https://www.youtube.com/watch?v=0CnRe0rtnys">video</a></b>]</p>
</td></tr></table>
<div id="footer">
<div id="footer-text">
Page generated 2021-03-16 05:39:25 EDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
