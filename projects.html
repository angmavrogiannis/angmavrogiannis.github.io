<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-151329879-1"></script>
<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'UA-151329879-1');
</script>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Projects</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="projects.html" class="current">Projects</a></div>
<div class="menu-item"><a href="coursework.html">Coursework</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="music.html">Music</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Projects</h1>
</div>
<h2>Human Driver Behavior Classification from Partial Trajectory Observation</h2>
<table class="imgtable"><tr><td>
<img src="images/clusters.png" alt="Behavioral Clusters" width="300px" />&nbsp;</td>
<td align="left"><p>My master thesis project at Carnegie Mellon is focused on human driver behavior classification. My hypothesis is that leveraging inherent cues representative of drivers&rsquo; personalities can lead to more accurate predictions for their future trajectories. I am using the Interstate 80 Freeway Dataset from the <b><a href="https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm">Next Generation Simulation (NGSIM)</a></b> project for which I developed a visualizer on python.<br />
Here is an outline for my approach:
<br /></p>
<ul>
<li><p>Probabilistic modeling</p>
</li>
<li><p>Data processing</p>
</li>
<li><p>Dimensionality reduction (PCA)</p>
</li>
<li><p>Behavioral clustering (k-means)</p>
</li>
<li><p>Online inference (KNN, Logistic Regression, Multilayer Perceptron)</p>
</li>
<li><p>Trajectory prediction (HMM)</p>
</li>
</ul>
<p>[<b><a href="https://www.youtube.com/watch?v=ldNasL2I--A">video</a></b>] [<b><a href="https://github.com/angmavrogiannis/Human-Driver-Behavior-Classification-from-Partial-Trajectory-Observation">code</a></b>]</p>
</td></tr></table>
<h2>Vehicle Behavior Prediction</h2>
<table class="imgtable"><tr><td>
<img src="images/semantic_feat.jpg" alt="Semantic Features" width="300px" />&nbsp;</td>
<td align="left"><p>Having joined Intelligent Control Lab at the Robotics Institute on spring 2019, I started working on vehicle behavior prediction, advised by Prof. <b><a href="https://www.cs.cmu.edu/~cliu6/">Changliu Liu</a></b>. In particular, I labeled vehicle trajectory data taken from the <b><a href="https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm">Next Generation Simulation (NGSIM)</a></b> dataset based on their maneuver attempted or executed during the next 3 seconds (keep current lane, turn left or turn right). I formulated a multiclass classification problem of predicting the intention/maneuver of a vehicle for a predetermined time horizon using semantic features, such as vehicle coordinates and velocity, and implemented a Long Short-Term Memory (LSTM) network using <b><a href="https://pytorch.org/">PyTorch</a></b> that outputs the probability of each class at every time step. I also demonstrated the superiority of LSTM networks on sequential prediction tasks by comparing the model to a regular feedforward network. [<b><a href="https://github.com/angmavrogiannis/Vehicle-Intention-Prediction/tree/master">code</a></b>]</p>
</td></tr></table>
<h2>Autonomous Vehicle Controller Design</h2>
<table class="imgtable"><tr><td>
<img src="images/route.png" alt="CMU buggy route" width="300px" />&nbsp;</td>
<td align="left"><p>As part of the course <b><a href="https://www.meche.engineering.cmu.edu/education/courses/24-677.html">Linear Control Systems</a></b>, taught by Prof. <b><a href="https://www.meche.engineering.cmu.edu/directory/bios/zhao-ding.html">Ding Zhao</a></b>, I designed a longitudinal and a lateral controller using different techniques to track the route of an autonomous <b><a href="https://www.cmu.edu/buggy/">buggy</a></b> vehicle around the CMU campus. A buggy simulator on python was used for getting the required response plots and tuning the parameters of each control method used. The longitudinal motion is controlled by a PID controller, while the lateral motion is controlled using:<br /></p>
<ul>
<li><p>A PID Controller</p>
</li>
<li><p>Pole Placement</p>
</li>
<li><p>Model Predictive Control</p>
</li>
<li><p>Kalman Filter</p>
</li>
</ul>
<p>The autonomous vehicle is required to achieve certain performance criteria, such as a minimum time to complete the route and an average and maximum deviation from the reference trajectory. In the end, a race was held and we competed with each other based on these criteria. [<b><a href="https://github.com/angmavrogiannis/24677-Linear-Control-Systems">code</a></b>]</p>
</td></tr></table>
<h2>Robot Design</h2>
<table class="imgtable"><tr><td>
<img src="images/oswald.png" alt="Oswald" width="300px" />&nbsp;</td>
<td align="left"><p>Drawing inspiration from penguins, I collaborated with a team of students to design, manufacture and test an underwater penguin robot for the course <b><a href="https://www.andrew.cmu.edu/user/amj1/classes/robotdesign.html">Robot Design &amp; Experimentation</a></b> taught by Prof. <b><a href="https://www.andrew.cmu.edu/user/amj1/">Aaron Johnson</a></b>. We came up with a ball-and-socket motion transmission mechanism for the movement of the flippers, fabricated a rib-and-spar body using 3D printers and laser-cutters and used Arduino for the controls. Besides contributing to the overall design and manufacturing process of the robot, I developed an underwater simulator on <b><a href="http://gazebosim.org/">gazebo</a></b> with the model of our constructed robot and used it to adjust the control parameters and perform tests before submerging the robot in the water. [<b><a href="https://www.youtube.com/watch?v=3IRqu0saWe8">video</a></b>] [<b><a href="docs/Oswald Final Project Report.pdf">report</a></b>] [<b><a href="docs/Mavrogiannis Final Executive Summary.pdf">executive summary</a></b>]</p>
</td></tr></table>
<h2>Game Design</h2>
<table class="imgtable"><tr><td>
<img src="images/sticky_man2.jpeg" alt="Sticky-Man game" width="300px" />&nbsp;</td>
<td align="left"><p>Using <b><a href="https://www.opengl.org/">OpenGL</a></b> on C++, we implemented a 2D fighting game for the course <b><a href="https://www.meche.engineering.cmu.edu/education/courses/24-780.html">Engineering Computation</a></b>, taught by Prof. <b><a href="https://engineering.cmu.edu/directory/bios/gomez-nestor.html">Nestor Gomez</a></b>. On the single player mode, the user controls a sticky-man figure and fights vs an AI agent. On the multiplayer mode, two users control their own sticky-man figure and fight until one of them is eliminated. The sticky-man figure can take a different set of states: gun mode (ranged), knife mode (melee), fight mode (melee). I created a menu for the game, as well as the background environment, enabling the players to actively interact with it (e.g. climbing stairs, jumping towards different floor levels). I also implemented a simple AI algorithm for the enemy player on the single player mode. [<b><a href="https://www.youtube.com/watch?v=kKTP6xleipg">video</a></b>] [<b><a href="https://github.com/angmavrogiannis/24-780B-Engineering-Computation/tree/master/Sticky-Man">code</a></b>]</p>
</td></tr></table>
<h2>Occluded Object Pose Estimation </h2>
<table class="imgtable"><tr><td>
<img src="images/object.png" alt="Egg-shaped object" width="300px" />&nbsp;</td>
<td align="left"><p>While manipulating objects in activities of daily living, we come across a problem where objects
are quite often severely occluded from the egocentric viewpoint making it difficult for the object
of interest to be tracked. Inspired by this problem, we collected a synthetic dataset of <b><a href="https://www.shadowrobot.com/products/dexterous-hand/">manipulator</a></b> postures and object poses in <b><a href="https://gym.openai.com/">OpenAI Gym</a></b> and mapped changes in hand pose to object displacements in order to track occluded objects. Our approach consists of a Multilayer Perceptron that takes as input the joint angles of the manipulator and outputs the position and rotation of the object. I worked on developing the neural network model and encoding the necessary input for the training process. I combined manipulator pose changes and previous object and manipulator positions into a vector, which gets fed to the network. The output of the network consists of the future positional coordinates and the quaternior (for rotation) of the predicted pose. This project was a part of the course <b><a href="https://www.cs.cmu.edu/afs/cs/academic/class/16741-s07/www/index.html">Mechanics of Manipulation</a></b>, taught by Prof. <b><a href="https://www.cs.cmu.edu/~mason/">Matthew Mason</a></b>. [<b><a href="docs/16-741 Term Project Report.pdf">report</a></b>]</p>
</td></tr></table>
<h2>Genetic-Algorithm-based Optimization Framework </h2>
<table class="imgtable"><tr><td>
<img src="images/gripper.png" alt="gripper-object forces" width="300px" />&nbsp;</td>
<td align="left"><p>For my undergraduate diploma thesis, I developed a framework on Visual Basic that receives mathematical expressions as input, analyzes them using a suitable parser, and optimizes them with genetic algorithms. The parser allows the input of the expressions in string format and distinguishes the variables, the parameters and the operational symbols. Besides the equations, the user can choose between a set of genetic algorithms for the optimization, as well as the hyperparameters. The implemented software was tested and validated on two applications: the minimization of the forces applied onto an object grasped by a robotic arm and the maximization of the stiffness of a cantilever beam. [<b><a href="https://github.com/angmavrogiannis/Diploma-Thesis">code</a></b>] [<b><a href="docs/DT_M_Mavrogiannis_Angelos_6387.pdf">thesis (in greek, abstract in english)</a></b>]</p>
</td></tr></table>
<h2>Computational Robotics Project</h2>
<table class="imgtable"><tr><td>
<img src="images/kuka_robot.jpg" alt="KUKA robot" width="300px" />&nbsp;</td>
<td align="left"><p>Forward and inverse kinematics constitute some of the most fundamental concepts in robotics. As part of an undergraduate robotics course, I chose a 6-degree-of-freedom industrial robot, namely <b><a href="https://www.kuka.com/en-us/products/robotics-systems/industrial-robots/kr-agilus?fbclid=IwAR1Z8iihzDCxpbKhjGFO33Bd0a9jV_eUrqWtp7ri1ybRV-G9Zaz1ItZgnGU">KUKA KR 6 R700 sixx WP</a></b>, upon which I developed software on MATLAB that does the following:
<br /></p>
<ul>
<li><p>Calculates the Denavit-Hartenberg parameters of the robot.</p>
</li>
<li><p>Implements the forward kinematics.</p>
</li>
<li><p>Implements the inverse kinematics.</p>
</li>
<li><p>Computes the Jacobian matrix.</p>
</li>
</ul>
<p>Finally, I applied the software to a trajectory planning application, combining linear and quadratic interpolation for two points in space. [<b><a href="https://github.com/angmavrogiannis/MEA-KY3-Robotics">code</a></b>]</p>
</td></tr></table>
<h2>2D Animation</h2>
<table class="imgtable"><tr><td>
<img src="images/pumpkins.jpeg" alt="Pumpkins" width="300px" />&nbsp;</td>
<td align="left"><p>For the individual project of the course <b><a href="https://www.meche.engineering.cmu.edu/education/courses/24-780.html">Engineering Computation</a></b> (taught by Prof. <b><a href="https://engineering.cmu.edu/directory/bios/gomez-nestor.html">Nestor Gomez</a></b>), I implemented a halloween-themed demo using <b><a href="https://www.opengl.org/">OpenGL</a></b> on C++. The demo includes 2D animation made with <b><a href="http://ysflight.in.coocan.jp/programming/fssimplewindow/e.html">this</a></b> rendering framework and music that I wrote, recorded and synchronized with the animation transitions. [<b><a href="https://www.youtube.com/watch?v=0CnRe0rtnys">video</a></b>] [<b><a href="https://github.com/angmavrogiannis/24-780B-Engineering-Computation/tree/master/Demo1">code</a></b>]</p>
</td></tr></table>
<div id="footer">
<div id="footer-text">
Page generated 2020-04-07 11:55:04 EDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
