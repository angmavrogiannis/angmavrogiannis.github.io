# jemdoc: menu{MENU}{projects.html}
= Projects

== B-GAP: Behavior-Guided Action Prediction for Autonomous Navigation
~~~
{}{img_left}{images/aggro.gif}{bgap}{300px}
We present a novel learning algorithm for action prediction and local navigation for autonomous driving. Our approach classifies the driver behavior of other vehicles or road-agents (aggressive or conservative) and takes that into account for decision making and safe driving. We present a behavior-driven simulator that can generate trajectories corresponding to different levels of aggressive behaviors and use our simulator to train a policy using graph convolutional networks. We use a reinforcement learning-based navigation scheme that uses a proximity graph of traffic agents and computes a safe trajectory for the ego-vehicle that accounts for aggressive driver maneuvers such as overtaking, over-speeding, weaving, and sudden lane changes. We have integrated our algorithm with *[https://gym.openai.com/ OpenAI gym]*-based *[https://github.com/eleurent/highway-env "Highway-Env"]* simulator and demonstrate the benefits in terms of improved navigation in different scenarios. Project supervised by Prof. *[https://scholar.google.com/citations?user=X08l_4IAAAAJ&hl=en&oi=ao Dinesh Manocha]* at *[https://gamma.umd.edu/ gamma group]*, *[https://www.umd.edu University of Maryland, College Park]*. \[*[https://arxiv.org/abs/2011.03748 arXiv]*\] \[*[https://gamma.umd.edu/bgap project page]*\] \[*[https://github.com/angmavrogiannis/B-GAP-Behavior-Guided-Action-Prediction-for-Autonomous-Navigation code]*\] \[*[https://youtu.be/AKa0esw88sQ video]*\]
~~~

== Human Driver Behavior Classification from Partial Trajectory Observation
~~~
{}{img_left}{images/ngsim_visualizer.png}{NGSIM Visualizer}{300px}
As autonomous vehicles are being tested on public roads, they must be able to share the road safely with human-driven vehicles. To ensure safety, autonomous vehicles must be capable of accurately estimating human drivers' intentions and their future trajectories. While there has been extensive research in this area, most of the existing approaches do not take into account the individual drivers' personalities and the patterns these personalities reflect on the trajectories of the vehicles. We tackle this issue by proposing a novel method of extracting high-level features from raw vehicle trajectory data and classifying drivers into behavioral classes based on their level of aggressiveness. We demonstrate how the identification of a driver's behavior improves the accuracy of the short-term trajectory prediction problem by introducing a prior knowledge on their behavior. Thesis supervised by Prof. *[https://www.cs.cmu.edu/~cliu6/ Changliu Liu]* at the *[http://icontrol.ri.cmu.edu/ Intelligent Control Lab]* in the *[https://www.ri.cmu.edu/ Robotics Institute]* of *[https://www.cmu.edu/ Carnegie Mellon University]*. \[*[https://www.researchgate.net/publication/345780499_Human_Driver_Behavior_Classification_from_Partial_Trajectory_Observation thesis]*\] \[*[https://github.com/angmavrogiannis/Human-Driver-Behavior-Classification-from-Partial-Trajectory-Observation code]*\] \[*[https://www.youtube.com/watch?v=ldNasL2I--A video]*\] \[*[https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm dataset]*\]
~~~

== Autonomous Vehicle Controller Design
~~~
{}{img_left}{images/route.png}{CMU buggy route}{300px}
As part of the course *[https://www.meche.engineering.cmu.edu/education/courses/24-677.html Linear Control Systems]*, taught by Prof. *[https://www.meche.engineering.cmu.edu/directory/bios/zhao-ding.html Ding Zhao]*, I designed a longitudinal and a lateral controller using different techniques to track the route of an autonomous *[https://www.cmu.edu/buggy/ buggy]* vehicle around the CMU campus. A buggy simulator on python was used for getting the required response plots and tuning the parameters of each control method used. The longitudinal motion is controlled by a PID controller, while the lateral motion is controlled using:\n
- A PID Controller
- Pole Placement
- Model Predictive Control
- Kalman Filter

The autonomous vehicle is required to achieve certain performance criteria, such as a minimum time to complete the route and an average and maximum deviation from the reference trajectory. In the end, a race was held and we competed with each other based on these criteria. \[*[https://github.com/angmavrogiannis/24677-Linear-Control-Systems code]*\]
~~~

== Robot Design
~~~
{}{img_left}{images/oswald.png}{Oswald}{300px}
Drawing inspiration from penguins, I collaborated with a team of students to design, manufacture and test an underwater penguin robot for the course *[https://www.andrew.cmu.edu/user/amj1/classes/robotdesign.html Robot Design & Experimentation]* taught by Prof. *[https://www.andrew.cmu.edu/user/amj1/ Aaron Johnson]*. We came up with a ball-and-socket motion transmission mechanism for the movement of the flippers, fabricated a rib-and-spar body using 3D printers and laser-cutters and used Arduino for the controls. Besides contributing to the overall design and manufacturing process of the robot, I developed an underwater simulator on *[http://gazebosim.org/ gazebo]* with the model of our constructed robot and used it to adjust the control parameters and perform tests before submerging the robot in the water. \[*{{<a href="docs/Oswald Final Project Report.pdf">report</a>}}*\] \[*{{<a href="docs/Mavrogiannis Final Executive Summary.pdf">executive summary</a>}}*\] \[*[https://www.youtube.com/watch?v=3IRqu0saWe8 video]*\]
~~~

== Game Design
~~~
{}{img_left}{images/sticky_man2.jpeg}{Sticky-Man game}{300px}
Using *[https://www.opengl.org/ OpenGL]* on C\+\+, we implemented a 2D fighting game for the course *[https://www.meche.engineering.cmu.edu/education/courses/24-780.html Engineering Computation]*, taught by Prof. *[https://engineering.cmu.edu/directory/bios/gomez-nestor.html Nestor Gomez]*. On the single player mode, the user controls a sticky-man figure and fights vs an AI agent. On the multiplayer mode, two users control their own sticky-man figure and fight until one of them is eliminated. The sticky-man figure can take a different set of states: gun mode (ranged), knife mode (melee), fight mode (melee). I created a menu for the game, as well as the background environment, enabling the players to actively interact with it (e.g. climbing stairs, jumping towards different floor levels). I also implemented a simple AI algorithm for the enemy player on the single player mode. \[*[https://github.com/angmavrogiannis/24-780B-Engineering-Computation/tree/master/Sticky-Man code]*\] \[*[https://www.youtube.com/watch?v=kKTP6xleipg video]*\]
~~~

== Occluded Object Pose Estimation 
~~~
{}{img_left}{images/object.png}{Egg-shaped object}{300px}
While manipulating objects in activities of daily living, we come across a problem where objects
are quite often severely occluded from the egocentric viewpoint making it difficult for the object
of interest to be tracked. Inspired by this problem, we collected a synthetic dataset of *[https://www.shadowrobot.com/products/dexterous-hand/ manipulator]* postures and object poses in *[https://gym.openai.com/ OpenAI Gym]* and mapped changes in hand pose to object displacements in order to track occluded objects. Our approach consists of a Multilayer Perceptron that takes as input the joint angles of the manipulator and outputs the position and rotation of the object. I worked on developing the neural network model and encoding the necessary input for the training process. I combined manipulator pose changes and previous object and manipulator positions into a vector, which gets fed to the network. The output of the network consists of the future positional coordinates and the quaternior (for rotation) of the predicted pose. This project was a part of the course *[https://www.cs.cmu.edu/afs/cs/academic/class/16741-s07/www/index.html Mechanics of Manipulation]*, taught by Prof. *[https://www.cs.cmu.edu/~mason/ Matthew Mason]*. \[*{{<a href="docs/16-741 Term Project Report.pdf">report</a>}}*\]
~~~

== Genetic-Algorithm-based Optimization Framework 
~~~
{}{img_left}{images/gripper.png}{gripper-object forces}{300px}
For my undergraduate diploma thesis, I developed a framework on Visual Basic that receives mathematical expressions as input, analyzes them using a suitable parser, and optimizes them with genetic algorithms. The parser allows the input of the expressions in string format and distinguishes the variables, the parameters and the operational symbols. Besides the equations, the user can choose between a set of genetic algorithms for the optimization, as well as the hyperparameters. The implemented software was tested and validated on two applications: the minimization of the forces applied onto an object grasped by a robotic arm and the maximization of the stiffness of a cantilever beam. \[*[https://github.com/angmavrogiannis/Diploma-Thesis code]*\] \[*{{<a href="docs/DT_M_Mavrogiannis_Angelos_6387.pdf">thesis (in greek, abstract in english)</a>}}*\]
~~~

== Computational Robotics Project
~~~
{}{img_left}{images/kuka_robot.jpg}{KUKA robot}{300px}
Forward and inverse kinematics constitute some of the most fundamental concepts in robotics. As part of an undergraduate robotics course, I chose a 6-degree-of-freedom industrial robot, namely *[https://www.kuka.com/en-us/products/robotics-systems/industrial-robots/kr-agilus?fbclid=IwAR1Z8iihzDCxpbKhjGFO33Bd0a9jV_eUrqWtp7ri1ybRV-G9Zaz1ItZgnGU KUKA KR 6 R700 sixx WP]*, upon which I developed software on MATLAB that does the following:
\n
- Calculates the Denavit-Hartenberg parameters of the robot.
- Implements the forward kinematics.
- Implements the inverse kinematics.
- Computes the Jacobian matrix.

Finally, I applied the software to a trajectory planning application, combining linear and quadratic interpolation for two points in space. \[*[https://github.com/angmavrogiannis/MEA-KY3-Robotics code]*\]
~~~

== 2D Animation
~~~
{}{img_left}{images/pumpkins.jpeg}{Pumpkins}{300px}
For the individual project of the course *[https://www.meche.engineering.cmu.edu/education/courses/24-780.html Engineering Computation]* (taught by Prof. *[https://engineering.cmu.edu/directory/bios/gomez-nestor.html Nestor Gomez]*), I implemented a halloween-themed demo using *[https://www.opengl.org/ OpenGL]* on C\+\+. The demo includes 2D animation made with *[http://ysflight.in.coocan.jp/programming/fssimplewindow/e.html this]* rendering framework and music that I wrote, recorded and synchronized with the animation transitions. \[*[https://github.com/angmavrogiannis/24-780B-Engineering-Computation/tree/master/Demo1 code]*\] \[*[https://www.youtube.com/watch?v=0CnRe0rtnys video]*\]
~~~
