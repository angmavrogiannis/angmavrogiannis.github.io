# jemdoc: menu{MENU}{projects.html}
= Projects

== Vehicle Behavior Prediction
~~~
{}{img_left}{images/semantic_feat.jpg}{Semantic Features}{300px}
Having joined Intelligent Control Lab at the Robotics Institute on spring 2019, I started working on vehicle behavior prediction, advised by Prof. *[https://www.cs.cmu.edu/~cliu6/ Changliu Liu]*. In particular, I labeled vehicle trajectory data taken from the *[https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm Next Generation Simulation (NGSIM)]* dataset based on their maneuver attempted or executed during the next 3 seconds (keep current lane, turn left or turn right). I formulated a multiclass classification problem of predicting the intention/maneuver of a vehicle for a predetermined time horizon using semantic features, such as vehicle coordinates and velocity, and implemented a Long Short-Term Memory (LSTM) network using *[https://pytorch.org/ PyTorch]* that outputs the probability of each class at every time step. I also demonstrated the superiority of LSTM networks on sequential prediction tasks by comparing the model to a regular feedforward network. \[*[https://github.com/angmavrogiannis/Vehicle-Intention-Prediction/tree/master code]*\]
~~~

== Robot Design
~~~
{}{img_left}{images/oswald.png}{Oswald}{300px}
Drawing inspiration from penguins, I collaborated with a team of students to design, manufacture and test an underwater penguin robot for the course *[https://www.andrew.cmu.edu/user/amj1/classes/robotdesign.html Robot Design & Experimentation]* taught by Prof. *[https://www.andrew.cmu.edu/user/amj1/ Aaron Johnson]*. We came up with a ball-and-socket motion transmission mechanism for the movement of the flippers, fabricated a rib-and-spar body using 3D printers and laser-cutters and used Arduino for the controls. Besides contributing to the overall design and manufacturing process of the robot, I developed an underwater simulator on *[https://gazebosim.org/ gazebo]* with the model of our constructed robot and used it to adjust the control parameters and perform tests before submerging the robot in the water. \[*[https://www.youtube.com/watch?v=3IRqu0saWe8 video]*\] \[*{{<a href="docs/Oswald Final Project Report.pdf">report</a>}}*\] \[*{{<a href="docs/Mavrogiannis Final Executive Summary.pdf">executive summary</a>}}*\]
~~~

== Game Design
~~~
{}{img_left}{images/sticky_man2.jpeg}{Sticky-Man game}{300px}
Using *[https://www.opengl.org/ OpenGL]* on C\+\+, we implemented a 2D fighting game for the course *[https://www.meche.engineering.cmu.edu/education/courses/24-780.html Engineering Computation]*, taught by Prof. *[https://engineering.cmu.edu/directory/bios/gomez-nestor.html Nestor Gomez]*. On the single player mode, the user controls a sticky-man figure and fights vs an AI agent. On the multiplayer mode, two users control their own sticky-man figure and fight until one of them is eliminated. The sticky-man figure can take a different set of states: gun mode (ranged), knife mode (melee), fight mode (melee). I created a menu for the game, as well as the background environment, enabling the players to actively interact with it (e.g. climbing stairs, jumping towards different floor levels). I also implemented a simple AI algorithm for the enemy player on the single player mode. \[*[https://www.youtube.com/watch?v=kKTP6xleipg video]*\] \[*[https://github.com/angmavrogiannis/24-780B-Engineering-Computation/tree/master/Sticky-Man code]*\]
~~~

== Occluded Object Pose Estimation 
~~~
{}{img_left}{images/object.png}{Egg-shaped object}{300px}
While manipulating objects in activities of daily living, we come across a problem where objects
are quite often severely occluded from the egocentric viewpoint making it difficult for the object
of interest to be tracked. Inspired by this problem, we collected a synthetic dataset of *[https://www.shadowrobot.com/products/dexterous-hand/ manipulator]* postures and object poses in *[https://gym.openai.com/ OpenAI Gym]* and mapped changes in hand pose to object displacements in order to track occluded objects. Our approach consists of a Multilayer Perceptron that takes as input the joint angles of the manipulator and outputs the position and rotation of the object. I worked on developing the neural network model and encoding the necessary input for the training process. I combined manipulator pose changes and previous object and manipulator positions into a vector, which gets fed to the network. The output of the network consists of the future positional coordinates and the quaternior (for rotation) of the predicted pose. This project was a part of the course *[https://www.cs.cmu.edu/afs/cs/academic/class/16741-s07/www/index.html Mechanics of Manipulation]*, taught by Prof. *[https://www.cs.cmu.edu/~mason/ Matthew Mason]*. \[*{{<a href="docs/16-741 Term Project Report.pdf">report</a>}}*\]
~~~

== Genetic-Algorithm-based Optimization Framework 
~~~
{}{img_left}{images/gripper.png}{gripper-object forces}{300px}
For my undergraduate diploma thesis, I developed a framework on Visual Basic that receives mathematical expressions as input, analyzes them using a suitable parser, and optimizes them with genetic algorithms. The parser allows the input of the expressions in string format and distinguishes the variables, the parameters and the operational symbols. Besides the equations, the user can choose between a set of genetic algorithms for the optimization, as well as the hyperparameters. The implemented software was tested and validated on two applications: the minimization of the forces applied onto an object grasped by a robotic arm and the maximization of the stiffness of a cantilever beam. \[*[https://github.com/angmavrogiannis/Diploma-Thesis code]*\] \[*{{<a href="docs/DT_M_Mavrogiannis_Angelos_6387.pdf">thesis (in greek, abstract in english)</a>}}*\]
~~~

== Computational Robotics Project
~~~
{}{img_left}{images/kuka_robot.jpg}{KUKA robot}{300px}
Forward and inverse kinematics constitute some of the most fundamental concepts in robotics. As part of an undergraduate robotics course, I chose a 6-degree-of-freedom industrial robot, namely *[https://www.kuka.com/en-us/products/robotics-systems/industrial-robots/kr-agilus?fbclid=IwAR1Z8iihzDCxpbKhjGFO33Bd0a9jV_eUrqWtp7ri1ybRV-G9Zaz1ItZgnGU KUKA KR 6 R700 sixx WP]*, upon which I developed software on MATLAB that does the following:
\n
- Calculates the Denavit-Hartenberg parameters of the robot.
- Implements the forward kinematics.
- Implements the inverse kinematics.
- Computes the Jacobian matrix.

Finally, I applied the software to a trajectory planning application, combining linear and quadratic interpolation for two points in space. \[*[https://github.com/angmavrogiannis/MEA-KY3-Robotics code]*\]
~~~

== Demo
~~~
{}{img_left}{images/pumpkins.jpeg}{Pumpkins}{300px}
For the individual project of the course *[https://www.meche.engineering.cmu.edu/education/courses/24-780.html Engineering Computation]* (taught by Prof. *[https://engineering.cmu.edu/directory/bios/gomez-nestor.html Nestor Gomez]*), I implemented a halloween-themed demo using *[https://www.opengl.org/ OpenGL]* on C\+\+. The demo includes 2D animation made with *[https://ysflight.in.coocan.jp/programming/fssimplewindow/e.html this]* rendering framework and music that I wrote, recorded and synchronized with the animation transitions. \[*[https://www.youtube.com/watch?v=0CnRe0rtnys video]*\] \[*[https://github.com/angmavrogiannis/24-780B-Engineering-Computation/tree/master/Demo1 code]*\]
~~~
